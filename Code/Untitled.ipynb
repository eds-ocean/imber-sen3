{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b96d1715",
   "metadata": {},
   "source": [
    "# START\n",
    "\n",
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc06dc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil, gc, glob\n",
    "\n",
    "import planetary_computer\n",
    "import pystac_client\n",
    "import fsspec\n",
    "\n",
    "import hda\n",
    "\n",
    "from getpass import getpass\n",
    "\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import cf_xarray, rioxarray\n",
    "\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from cdo import *\n",
    "cdo = Cdo()\n",
    "cdo.cleanTempDir()\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import colors\n",
    "from matplotlib import cm\n",
    "import colormaps as cmo\n",
    "\n",
    "from cartopy import crs as ccrs\n",
    "from cartopy import feature as cf\n",
    "\n",
    "import zipfile\n",
    "\n",
    "from rich.table import Table\n",
    "from rich.markdown import Markdown\n",
    "from rich.console import Console\n",
    "console = Console()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35c63fc",
   "metadata": {},
   "source": [
    "## Create directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf2f348",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_dir = os.path.join(os.getcwd(),\"sentinel-3_program\",\"downloaded-data\")\n",
    "result_dir = os.path.join(os.getcwd(),\"sentinel-3_program\",\"processed-data\")\n",
    "\n",
    "os.makedirs(download_dir, exist_ok=True)\n",
    "os.makedirs(result_dir, exist_ok=True)\n",
    "\n",
    "cleardown = glob.glob(os.path.join(download_dir, \"*\"))\n",
    "\n",
    "for file in cleardown:\n",
    "    os.remove(file)\n",
    "\n",
    "clearres = glob.glob(os.path.join(result_dir, \"*\"))\n",
    "clearres = [f for f in clearres if \"Sen-3\" not in f]\n",
    "\n",
    "for file in clearres:\n",
    "    os.remove(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024cbbd5",
   "metadata": {},
   "source": [
    "## Create flags definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f00334",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_flags_common = ['LAND','INLAND_WATER','COASTLINE','CLOUD','CLOUD_AMBIGUOUS','CLOUD_MARGIN','INVALID','COSMETIC','SATURATED','SUSPECT','HISOLZEN','HIGHGLINT','SNOW_ICE']\n",
    "list_flags_process = ['AC_FAIL','WHITECAPS','ADJAC','RWNEG_O2','RWNEG_O3','RWNEG_O4','RWNEG_O5','RWNEG_O6','RWNEG_O7','RWNEG_O8']\n",
    "list_flags_oc4me = ['OC4ME_FAIL','TIDAL']\n",
    "list_flags_ocnn = ['OCNN_FAIL']\n",
    "\n",
    "def flag_data_fast(list_flag, flag_names, flag_values, flag_data, flag_type='WQSF'):\n",
    "    flag_bits = np.uint64()\n",
    "    if flag_type == 'SST':\n",
    "        flag_bits = np.uint8()\n",
    "    elif flag_type == 'WQSF_lsb':\n",
    "        flag_bits = np.uint32()\n",
    "    for flag in list_flag:\n",
    "        try:\n",
    "            flag_bits = flag_bits | flag_values[flag_names.index(flag)]\n",
    "        except:\n",
    "            print(flag + ' not present')\n",
    "    return (flag_data & flag_bits) > 0\t\t\t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8125f8eb",
   "metadata": {},
   "source": [
    "# Location\n",
    "\n",
    "## Input location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7956748",
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(1)\n",
    "os.system('cls' if os.name == 'nt' else 'clear') \n",
    "\n",
    "area_md = '''\n",
    "### üåè  Area of Interest\n",
    "Please input coordinates of your area of interest. The coordinates should be in **decimal format** with minus (‚ûñ) sign for south-of-equator latitude or west-of-greenwich longitude.\n",
    "'''\n",
    "# Area of interest\n",
    "console.print(Markdown(area_md))\n",
    "\n",
    "north = float(input('North point: ')) # -6.85\n",
    "south = float(input('South point: ')) # -7.95\n",
    "west = float(input('West point: ')) # 112.66\n",
    "east = float(input('East point: ')) # 114.65\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a293e0f",
   "metadata": {},
   "source": [
    "## Create dummy location file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937cb986",
   "metadata": {},
   "outputs": [],
   "source": [
    "area_of_interest = {\n",
    "    \"type\": \"Polygon\",\n",
    "    \"coordinates\": [\n",
    "        [\n",
    "            [west, south],\n",
    "            [east, south],\n",
    "            [east, north],\n",
    "            [west, north],\n",
    "            [west, south],\n",
    "        ]\n",
    "    ],\n",
    "}\n",
    "\n",
    "\n",
    "extent = [west, east, south, north]\n",
    "bbox = [west, south, east, north]\n",
    "\n",
    "bbox_str = f'{west},{east},{south},{north}' \n",
    "\n",
    "def format_coordinate(value, is_latitude):\n",
    "    if is_latitude:\n",
    "        suffix = \"S\" if value < 0 else \"N\"\n",
    "    else:\n",
    "        suffix = \"W\" if value < 0 else \"E\"\n",
    "\n",
    "    return f\"{abs(value)}{suffix}\"\n",
    "\n",
    "north_str = format_coordinate(north, is_latitude=True)\n",
    "south_str = format_coordinate(south, is_latitude=True)\n",
    "west_str = format_coordinate(west, is_latitude=False)\n",
    "east_str = format_coordinate(east, is_latitude=False)\n",
    "\n",
    "geostr = f\"{north_str}_{south_str}_{west_str}_{east_str}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa19f8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dummy dataset based on the area of interest\n",
    "resolution = 300\n",
    "resolution_degrees = resolution / 111320\n",
    "\n",
    "num_lon = int(np.ceil((east - west) / resolution_degrees)) + 1\n",
    "num_lat = int(np.ceil((north - south) / resolution_degrees)) + 1\n",
    "\n",
    "lon = np.linspace(west, east, num_lon)\n",
    "lat = np.linspace(south, north, num_lat)\n",
    "\n",
    "ds = xr.Dataset(\n",
    "    coords={\n",
    "        \"lon\": ([\"lon\"], lon),\n",
    "        \"lat\": ([\"lat\"], lat),\n",
    "    }\n",
    ")\n",
    "\n",
    "ds.lat.attrs = {\n",
    "    'units' : 'degrees_north',\n",
    "    'unit_long' : \"Degrees North\",\n",
    "    'standard_name' : \"latitude\",\n",
    "    'long_name' : \"Latitude\",\n",
    "    'axis' : 'Y'\n",
    "}\n",
    "\n",
    "ds.lon.attrs = {\n",
    "    'units' : 'degrees_east',\n",
    "    'unit_long' : \"Degrees East\",\n",
    "    'standard_name' : \"longitude\",\n",
    "    'long_name' : \"Longitude\",\n",
    "    'axis' : 'X'\n",
    "}\n",
    "\n",
    "ds[\"data\"] = ([\"lat\", \"lon\"], np.zeros((num_lat, num_lon)))\n",
    "\n",
    "ds.rio.write_crs('epsg:4326', inplace=True)\n",
    "\n",
    "ds.to_netcdf(download_dir + '/grid_data.nc')\n",
    "\n",
    "dsinput = download_dir + '/grid_data.nc'\n",
    "grids = cdo.griddes(input = dsinput)\n",
    "gridfile = os.path.join(os.getcwd(), 'gridfile.txt') \n",
    "\n",
    "with open(gridfile, 'w') as f:\n",
    "    print(\"\\n\".join(line.strip(\"'\") for line in grids), file = f)\n",
    "\n",
    "\n",
    "ds.close()\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d0aabf",
   "metadata": {},
   "source": [
    "# Input datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde54d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(1)\n",
    "os.system('cls' if os.name == 'nt' else 'clear') \n",
    "\n",
    "## Time of interest\n",
    "time_md = '''\n",
    "### üïí  Time of Interest\n",
    "\n",
    "Please input the start and end of your time of interest. The dates should be in `YYYY-MM-DD` format. This program will select data available between the the times, and returned dataset time might be different due to the availability.\n",
    "'''\n",
    "\n",
    "console.print(Markdown(time_md))\n",
    "\n",
    "print()\n",
    "dtstart = input('Time start: ')\n",
    "dtend = input('Time end: ')\n",
    "\n",
    "time_of_interest = f\"{dtstart}/{dtend}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba614ea3",
   "metadata": {},
   "source": [
    "# Input datatype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177df9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(1)\n",
    "os.system('cls' if os.name == 'nt' else 'clear') \n",
    "\n",
    "params_md = '''\n",
    "### üìä  Dataset Parameter\n",
    "\n",
    "Please select dataset parameters you want to download. \n",
    "\n",
    "1. Download geophysical parameters (chlorophyll-a and total suspended matter)\n",
    "2. Download water surface reflectances.\n",
    "'''\n",
    "\n",
    "console.print(Markdown(params_md))\n",
    "\n",
    "print()\n",
    "\n",
    "while True:\n",
    "    parameters = int(input('Parameters: '))\n",
    "\n",
    "    if parameters == 1:\n",
    "        nick = 'geophysical-data'\n",
    "        print()\n",
    "        print('Geophysical parameters will be processed.')\n",
    "        break\n",
    "    elif parameters == 2:\n",
    "        nick = 'optical-data'\n",
    "        print()\n",
    "        print('Water surface reflectances will be processed.')\n",
    "        break\n",
    "    else:\n",
    "        print(\"You put wrong number. Please try again!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63208ce8",
   "metadata": {},
   "source": [
    "# Select data source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29355fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    datasources = int(input('Dataset source: '))\n",
    "\n",
    "    if datasources == 1:\n",
    "        nick = 'planetary'\n",
    "\n",
    "        catalog = pystac_client.Client.open(\"https://planetarycomputer.microsoft.com/api/stac/v1\", modifier=planetary_computer.sign_inplace)\n",
    "        search = catalog.search(collections=[\"sentinel-3-olci-wfr-l2-netcdf\"], intersects=area_of_interest, datetime=time_of_interest)\n",
    "\n",
    "        items = search.item_collection()\n",
    "\n",
    "        table = Table(title = \"Summary\")\n",
    "\n",
    "        table.add_column(\"Released\", justify=\"left\", style=\"cyan\", no_wrap=True)\n",
    "        table.add_column(\"Title\", justify=\"left\", style=\"magenta\")\n",
    "\n",
    "        table.add_row(f\"Area of interest\",f\"{geostr}\")\n",
    "        table.add_row(f\"Time of interest\",f\"{time_of_interest}\")\n",
    "        table.add_row(f\"Number of items\",f\"{len(items)}\")\n",
    "        table.add_row(f\"First dataset\",f\"{items[-1].properties['datetime']}\")\n",
    "        table.add_row(f\"last dataset\",f\"{items[0].properties['datetime']}\")\n",
    "\n",
    "        console.print(table)\n",
    "\n",
    "        print()\n",
    "\n",
    "        for i in range(3, 0, -1):\n",
    "            print(f\"Process will be started in ... {i} \", end=\"\\r\", flush=True)\n",
    "            time.sleep(1) \n",
    "\n",
    "        os.system('cls' if os.name == 'nt' else 'clear') \n",
    "\n",
    "        for index, item in tqdm(enumerate(items, start=1), desc=\"Processing: \", total = len(items), position=0, leave=False):\n",
    "            try:\n",
    "                console.log(f'Processing data #{index} started.')\n",
    "                file_id = item.id\n",
    "            \n",
    "                date_string = item.properties['datetime']\n",
    "                timestamp = datetime.strptime(date_string, '%Y-%m-%dT%H:%M:%S.%fZ')\n",
    "                \n",
    "                console.log(f'Dataset #{index}: Reading.')\n",
    "                coord_file = xr.open_dataset(fsspec.open(item.assets[\"geo-coordinates\"].href).open())\n",
    "                flags_file = xr.open_dataset(fsspec.open(item.assets[\"wqsf\"].href).open())\n",
    "                \n",
    "                flag_names = flags_file['WQSF'].flag_meanings.split(' ')\n",
    "                flag_vals = flags_file['WQSF'].flag_masks\n",
    "                flags_data = flags_file.variables['WQSF'].data\n",
    "            \n",
    "                dta = xr.Dataset()\n",
    "                dta['longitude'] = coord_file['longitude']\n",
    "                dta['latitude'] = coord_file['latitude']\n",
    "                \n",
    "                coord_file.close()\n",
    "                flags_file.close()\n",
    "                gc.collect()\n",
    "            \n",
    "                console.log(f'Dataset #{index}: Applying flags.')\n",
    "            \n",
    "                if parameters == 1:\n",
    "                    keys = [\"chl-nn\",\"tsm-nn\",\"chl-oc4me\"]\n",
    "                    for k in keys:\n",
    "                        if not k == 'chl_oc4me':\n",
    "                            list_flags = list_flags_common + list_flags_ocnn\n",
    "                        else:\n",
    "                            list_flags = list_flags_common + list_flags_process + list_flags_oc4me\n",
    "                \n",
    "                        ds = xr.open_dataset(fsspec.open(item.assets[k].href).open())\n",
    "                        da = ds[[var for var in ds.variables if \"_err\" not in var][0]]\n",
    "                        ds.close()\n",
    "                \n",
    "                        dtarr = da.data\n",
    "                \n",
    "                        flag_mask = flag_data_fast(list_flags, flag_names, flag_vals, flags_data, flag_type='WQSF')\n",
    "                        \n",
    "                        flagged = np.where(flag_mask, np.nan, dtarr)\n",
    "                        \n",
    "                        dta[str(k)] = xr.DataArray(flagged, dims=('rows','columns'))\n",
    "                        dta[str(k)].attrs = da.attrs\n",
    "                \n",
    "                        del ds\n",
    "                        del da\n",
    "                        del dtarr\n",
    "                \n",
    "                elif parameters == 2:\n",
    "                    keys = ['Oa01-reflectance','Oa02-reflectance','Oa03-reflectance','Oa04-reflectance','Oa05-reflectance','Oa06-reflectance','Oa07-reflectance','Oa08-reflectance','Oa09-reflectance','Oa10-reflectance','Oa11-reflectance','Oa12-reflectance','Oa16-reflectance','Oa17-reflectance','Oa18-reflectance','Oa21-reflectance']\n",
    "                    list_flags = list_flags_common + list_flags_process\n",
    "                    for k in keys:\n",
    "                        ds = xr.open_dataset(fsspec.open(item.assets[k].href).open())\n",
    "                        da = ds[[var for var in ds.variables if \"_err\" not in var][0]]\n",
    "                        ds.close()\n",
    "                \n",
    "                        dtarr = da.data\n",
    "                \n",
    "                        flag_mask = flag_data_fast(list_flags, flag_names, flag_vals, flags_data, flag_type='WQSF')\n",
    "                        \n",
    "                        flagged = np.where(flag_mask, np.nan, dtarr)\n",
    "                        \n",
    "                        dta[str(k)] = xr.DataArray(flagged, dims=('rows','columns'))\n",
    "                        dta[str(k)].attrs = da.attrs\n",
    "                \n",
    "                        del ds\n",
    "                        del da\n",
    "                        del dtarr\n",
    "                \n",
    "                dta = dta.set_coords(['latitude','longitude'])\n",
    "                dta = dta.expand_dims(dim={\"time\":[timestamp]}, axis=0)\n",
    "            \n",
    "                console.log(f'Dataset #{index}: Subsetting.')\n",
    "            \n",
    "                reggrid = cdo.sellonlatbox(bbox_str, input = dta, returnXDataset = True)\n",
    "                \n",
    "                comp = dict(zlib=True, _FillValue=-99999.0, complevel=4)\n",
    "                encoding = {var: comp for var in reggrid.data_vars}\n",
    "                \n",
    "                reggrid.to_netcdf(\n",
    "                    os.path.join(download_dir , file_id + f'_{nick}.nc'),\n",
    "                    format='NETCDF4', \n",
    "                    unlimited_dims=['time'],\n",
    "                    encoding=encoding\n",
    "                )\n",
    "                \n",
    "                dta.close()\n",
    "                del reggrid\n",
    "                cdo.cleanTempDir()\n",
    "                gc.collect()\n",
    "            \n",
    "                console.log(f'Dataset #{index}: Regridding.')\n",
    "            \n",
    "                dataset = xr.open_dataset(os.path.join(download_dir , file_id + f'_{nick}.nc'), decode_coords=\"all\")\n",
    "                dataset = dataset.cf.add_bounds(['latitude','longitude'])\n",
    "                \n",
    "                reggridded = cdo.remapcon(gridfile, input = dataset, returnXDataset = True)\n",
    "                \n",
    "                comp = dict(zlib=True, _FillValue=-99999.0, complevel=4)\n",
    "                encoding = {var: comp for var in reggridded.data_vars}\n",
    "                \n",
    "                reggridded.to_netcdf(\n",
    "                    os.path.join(result_dir , file_id + f'_{nick}.nc'),\n",
    "                    format='NETCDF4', \n",
    "                    unlimited_dims=['time'],\n",
    "                    encoding=encoding\n",
    "                )\n",
    "            \n",
    "                dataset.close()\n",
    "                del reggridded\n",
    "                cdo.cleanTempDir()\n",
    "                gc.collect()\n",
    "            \n",
    "                console.log(f'Dataset #{index}: Finished')\n",
    "            \n",
    "                time.sleep(0.5)\n",
    "                os.system('cls' if os.name == 'nt' else 'clear') \n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        break\n",
    "\n",
    "    elif datasources == 2:\n",
    "        nick = 'wekeo'\n",
    "        print()\n",
    "        while True:\n",
    "            try:\n",
    "                user = input(\"Enter your WEkEO username: \")\n",
    "                passw = getpass(\"Enter your WEkEO password: \")\n",
    "                c = hda.Client(hda.Configuration(user=user, password=passw), progress=True, max_workers=1)\n",
    "                print()\n",
    "                print(f\"Login successfull!. Your token is {c.token}.\")\n",
    "                break\n",
    "            except KeyError:\n",
    "                print()\n",
    "                print('You entered wrong username and/or password.')\n",
    "\n",
    "        sat_md_1 = '''\n",
    "        ### Enter Satellite Parameters\n",
    "\n",
    "        Sentinel-3 program have 2 satellites: Sentinel-3A (launched 16 February 2016) and Sentinel-3B (launched 25 April 2018).\n",
    "\n",
    "        However, the Sentinel-3 Level 2 dataset in WEkEO are available in two type:\n",
    "\n",
    "        1. `EO:EUM:DAT:SENTINEL-3:0556` &rarr; Reprocessed dataset (25 April 2016 - 28 April 2021)\n",
    "        2. `EO:EUM:DAT:SENTINEL-3:OL_2_WFR___` &rarr; (5 July 2017 - recent)\n",
    "\n",
    "        #### Enter the satellite designation (`A` or `B`):\n",
    "\n",
    "        - `A` = Sentinel-3A\n",
    "        - `B` = Sentinel-3B\n",
    "\n",
    "        Leave it blank if you want both Sentinel-3A and Sentinel-3B queried.\n",
    "\n",
    "        '''\n",
    "\n",
    "        rprint(Markdown(sat_md_1))\n",
    "        print()\n",
    "\n",
    "        sat_nm = input(\"Satellite name: \")\n",
    "\n",
    "        print()\n",
    "\n",
    "        if sat_nm == 'a' or sat_nm == 'A':\n",
    "            sat = 'Sentinel-3A'\n",
    "            print()\n",
    "        elif sat_nm == 'b' or sat_nm == 'B':\n",
    "            sat = 'Sentinel-3B'\n",
    "            print()\n",
    "        else:\n",
    "            sat = ''\n",
    "            print()\n",
    "\n",
    "        sat_md_2 = '''\n",
    "\n",
    "        #### Enter Sentinel-3 dataset ID (`1` or `2`)\n",
    "\n",
    "        1. EO:EUM:DAT:SENTINEL-3:0556\n",
    "        2. EO:EUM:DAT:SENTINEL-3:OL_2_WFR___\n",
    "        '''\n",
    "\n",
    "        rprint(Markdown(sat_md_2))\n",
    "\n",
    "        print()\n",
    "\n",
    "        while True:\n",
    "            sat_id = int(input('Satellite ID: '))\n",
    "\n",
    "            if sat_id == 1:\n",
    "                dataset_id = 'EO:EUM:DAT:SENTINEL-3:0556'\n",
    "                print()\n",
    "                break\n",
    "            elif sat_id == 2:\n",
    "                dataset_id = 'EO:EUM:DAT:SENTINEL-3:OL_2_WFR___'\n",
    "                print()\n",
    "                break\n",
    "            else:\n",
    "                print()\n",
    "                rprint(\"You put wrong number. Please try again!\")\n",
    "\n",
    "        print()\n",
    "\n",
    "        for i in range(3, 0, -1):\n",
    "            print(f\"Process will be started in ... {i} \", end=\"\\r\", flush=True)\n",
    "            time.sleep(1) \n",
    "\n",
    "        os.system('cls' if os.name == 'nt' else 'clear') \n",
    "\n",
    "        for index, result in tqdm(enumerate(search_result.results, start=0), desc=\"Processing: \", total = len(search_result.results), position=0, leave=False):\n",
    "            try:\n",
    "                console.log(f'Processing data #{index} started.')\n",
    "                file_id = result['id']\n",
    "            \n",
    "                start = datetime.strptime(result['properties']['startdate'], '%Y-%m-%dT%H:%M:%S%fZ')\n",
    "                end = datetime.strptime(result['properties']['enddate'], '%Y-%m-%dT%H:%M:%S%fZ')\n",
    "                timestamp = start + (end - start) / 2\n",
    "            \n",
    "                console.log(f'Downloading data.')\n",
    "                search_result[index].download()\n",
    "            \n",
    "            \n",
    "                with zipfile.ZipFile(file_id + '.zip', 'r') as zip_ref:\n",
    "                    console.log(f'Extracting data.')\n",
    "                    zip_ref.extractall(download_dir)\n",
    "                    os.remove(file_id + '.zip')\n",
    "            \n",
    "                console.log(f'Applying mask to data.')\n",
    "                \n",
    "                geo_coords = xr.open_dataset(os.path.join(download_dir, file_id, 'geo_coordinates.nc'))\n",
    "                \n",
    "                flag_file = xr.open_dataset(os.path.join(download_dir, file_id, 'wqsf.nc'))\n",
    "                flag_names = flag_file['WQSF'].flag_meanings.split(' ') #flag names\n",
    "                flag_vals = flag_file['WQSF'].flag_masks #flag bit values\n",
    "                flags_data = flag_file.variables['WQSF'].data\n",
    "                    \n",
    "                dta = xr.Dataset()\n",
    "                dta['longitude'] = geo_coords['longitude']\n",
    "                dta['latitude'] = geo_coords['latitude']\n",
    "                \n",
    "                geo_coords.close()\n",
    "                flag_file.close()\n",
    "                gc.collect()\n",
    "            \n",
    "                if parameters == 1:\n",
    "                    keys = [\"chl_nn\",\"tsm_nn\",\"chl_oc4me\"]\n",
    "                    for k in keys:\n",
    "                        if not k == 'chl_oc4me':\n",
    "                            list_flags = list_flags_common + list_flags_ocnn\n",
    "                        else:\n",
    "                            list_flags = list_flags_common + list_flags_process + list_flags_oc4me\n",
    "                \n",
    "                        ds = xr.open_dataset(os.path.join(download_dir, file_id, f'{k}.nc'))\n",
    "                        dtarr = ds[str(k.upper())].data\n",
    "                        flag_mask = flag_data_fast(list_flags, flag_names, flag_vals, flags_data, flag_type='WQSF')\n",
    "                        \n",
    "                        flagged = np.where(flag_mask, np.nan, dtarr)\n",
    "                        \n",
    "                        dta[str(k)] = xr.DataArray(flagged, dims=('rows','columns'))\n",
    "                        dta[str(k)].attrs = ds[str(k.upper())].attrs\n",
    "                elif parameters == 2:\n",
    "                    keys = ['Oa01_reflectance','Oa02_reflectance','Oa03_reflectance','Oa04_reflectance','Oa05_reflectance','Oa06_reflectance','Oa07_reflectance','Oa08_reflectance','Oa09_reflectance','Oa10_reflectance','Oa11_reflectance','Oa12_reflectance','Oa16_reflectance','Oa17_reflectance','Oa18_reflectance','Oa21_reflectance']\n",
    "                    list_flags = list_flags_common + list_flags_process\n",
    "                    for k in keys:\n",
    "                        ds = xr.open_dataset(os.path.join(download_dir, file_id, f'{k}.nc'))\n",
    "                        dtarr = ds[str(k)].data\n",
    "                        flag_mask = flag_data_fast(list_flags, flag_names, flag_vals, flags_data, flag_type='WQSF')\n",
    "                        \n",
    "                        flagged = np.where(flag_mask, np.nan, dtarr)\n",
    "                        \n",
    "                        dta[str(k)] = xr.DataArray(flagged, dims=('rows','columns'))\n",
    "                        dta[str(k)].attrs = ds[str(k)].attrs\n",
    "                \n",
    "                dta = dta.set_coords(['latitude','longitude'])\n",
    "                dta = dta.expand_dims(dim={\"time\":[timestamp]}, axis=0)\n",
    "            \n",
    "                console.log(f'Subsetting data.')\n",
    "            \n",
    "                reggrid = cdo.sellonlatbox(bbox_str, input = dta, returnXDataset = True)\n",
    "                \n",
    "                comp = dict(zlib=True, _FillValue=-99999.0, complevel=4)\n",
    "                encoding = {var: comp for var in reggrid.data_vars}\n",
    "                \n",
    "                reggrid.to_netcdf(\n",
    "                    os.path.join(download_dir , file_id + f'_{nick}.nc'),\n",
    "                    format='NETCDF4', \n",
    "                    unlimited_dims=['time'],\n",
    "                    encoding=encoding\n",
    "                )\n",
    "                \n",
    "                cdo.cleanTempDir()\n",
    "            \n",
    "                dataset = xr.open_dataset(os.path.join(download_dir , file_id + f'_{nick}.nc'), decode_coords=\"all\")\n",
    "                dataset = dataset.cf.add_bounds(['latitude','longitude'])\n",
    "            \n",
    "                reggridded = cdo.remapcon(gridfile, input = dataset, returnXDataset = True)\n",
    "                \n",
    "                comp = dict(zlib=True, _FillValue=-99999.0, complevel=4)\n",
    "                encoding = {var: comp for var in reggridded.data_vars}\n",
    "                \n",
    "                reggridded.to_netcdf(\n",
    "                    os.path.join(result_dir , file_id + f'_{nick}.nc'),\n",
    "                    format='NETCDF4', \n",
    "                    unlimited_dims=['time'],\n",
    "                    encoding=encoding\n",
    "                )\n",
    "                \n",
    "                cdo.cleanTempDir()\n",
    "                \n",
    "                gc.collect()\n",
    "                \n",
    "                for allitem in os.listdir(download_dir):\n",
    "                    path = os.path.join(download_dir,allitem)\n",
    "                    if os.path.isfile(path):\n",
    "                        os.remove(path)\n",
    "                    elif os.path.isdir(path):\n",
    "                        shutil.rmtree(path)\n",
    "            \n",
    "                console.log(f'#{index + 1} data processing done.')\n",
    "            \n",
    "                del dta\n",
    "                del dataset\n",
    "                del reggridded\n",
    "            \n",
    "                time.sleep(1)\n",
    "                os.system('cls' if os.name == 'nt' else 'clear') \n",
    "            except:\n",
    "                continue\n",
    "        break\n",
    "    else:\n",
    "        print(\"You put wrong number. Please try again!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525d59b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "console.log(f'Combining dataset.')\n",
    "\n",
    "files = glob.glob(os.path.join(result_dir , f'S3*{nick}.nc'))\n",
    "ds = xr.open_mfdataset(files)\n",
    "\n",
    "ds_day = ds.resample(time=\"D\").mean()\n",
    "\n",
    "ds.close()\n",
    "gc.collect()\n",
    "\n",
    "console.log(f'Saving result.')\n",
    "\n",
    "ds_day.to_netcdf(\n",
    "    os.path.join(result_dir, f'Sen-3_{str(ds.time[0].data)[0:10]}_{str(ds.time[-1].data)[0:10]}_{geostr}_{nick}.nc'),\n",
    "    format = 'NETCDF4', \n",
    "    encoding = {var: comp for var in ds_day.data_vars}\n",
    ")\n",
    "\n",
    "console.log(f'All process done! üòé')\n",
    "\n",
    "time.sleep(3)\n",
    "\n",
    "md_end = f\"\"\"\n",
    "Processing requested dataset succesfull. You can now download your data by accessing this path `{os.path.join(result_dir, f'Sen-3_{str(ds.time[0].data)[0:10]}_{str(ds.time[-1].data)[0:10]}_{geostr}_{nick}.nc')}` from the sidebar.\n",
    "\"\"\"\n",
    "\n",
    "console.print(Markdown(md_end))\n",
    "\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa329f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_to_delete = glob.glob(os.path.join(result_dir, \"*.nc\"))\n",
    "files_to_delete = [f for f in files_to_delete if \"Sen-3\" not in f]\n",
    "\n",
    "for file in files_to_delete:\n",
    "    os.remove(file)\n",
    "\n",
    "#display(ds)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6a91b272",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sen3_pc",
   "language": "python",
   "name": "sen3_pc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
